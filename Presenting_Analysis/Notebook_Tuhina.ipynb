{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbcd586-7f0d-47df-8b86-fba42d743f32",
   "metadata": {},
   "source": [
    "# Data Analysis Workflow with Web Scraping, API Integration, and SQL\n",
    "This notebook demonstrates different approaches to solving challenges faced in the module. It includes examples of web scraping, API integration, SQL and Python integration, and their corresponding improvements. Each section is accompanied by explanations that describe the issues encountered and the solutions implemented to improve the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bb064-3ebb-4f34-8a37-2f303896f0e4",
   "metadata": {},
   "source": [
    "## 1. Web Scraping with Beautiful Soup\r\n",
    "**Explanation**: Initially, when I was trying to scrape data from a webpage, extracting all `<a>` tags yielded too much irrelevant information. For example, I only wanted to scrape links related to product categories, but the code returned all links on the page. The solution was to filter based on specific HTML class attributes.\r\n",
    "\r\n",
    "### Initial Attempt:\r\n",
    "This approach returned all `<a>` tags, leading to irrelevant dta.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edc7ad-a4f1-4973-901a-a523d81fa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send request to the website\n",
    "url = 'https://example.com/products'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract all <a> tags (initial attempt)\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163a221-5d1c-4f07-bd85-346f79ae6fed",
   "metadata": {},
   "source": [
    "**Challenge:** The code returned all links indiscriminately, leading to irrelevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326da098-90ef-43d7-bfa9-1f5919ba97dd",
   "metadata": {},
   "source": [
    "### Improved Solution:\r\n",
    "By filtering the `<a>` tags using the class attribute (`class_='product-category'`), I was able to extract only the relevant product category links, reducing the unnecessary data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc4c5dd-2c11-4cd1-9b1f-0559c3a2c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved code\n",
    "product_categories = soup.find_all('a', class_='product-category')\n",
    "for category in product_categories:\n",
    "    print(category.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad585fa-152a-4c1e-a845-192acc5dc744",
   "metadata": {},
   "source": [
    "**Reflection:** By narrowing down the search to a specific class attribute, I reduced the noise and only extracted the relevant links. This approach helped me avoid unnecessary data and made the scraping process much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905bf74-f928-4f34-baf1-1c5b7611bedf",
   "metadata": {},
   "source": [
    "## 2. API Integration with Error Handling\n",
    "**Explanation**: When working with APIs, failing requests due to rate limits or connectivity issues caused the program to crash. Initially, there was no error handling, which made the program unreliable. By adding error handling using `try-except` blocks, I prevented the program from crashing and ensured that proper error messages were returned.\n",
    "\n",
    "### Initial Attempt:\n",
    "This approach lacked error handling and crashed when an API request failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad099db-8e6c-4edc-ab68-23ccbee55d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Use the actual API key provided by OpenWeather (appid = API key)\n",
    "api_key = 'cff2c087740b5a979aaf71a1537b3f5d' \n",
    "\n",
    "# OpenWeather API endpoint\n",
    "api_url = 'https://api.openweathermap.org/data/2.5/weather'\n",
    "\n",
    "# Define the query parameters, including 'appid'\n",
    "params = {\n",
    "    'q': 'London',     # City for which you want weather data\n",
    "    'appid': 'cff2c087740b5a979aaf71a1537b3f5d',  # Your API key (appid)\n",
    "    'units': 'metric'  # Optional: Set units (e.g., metric for Celsius)\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fefa14c-e909-4909-84f2-a67d9ffdb1d3",
   "metadata": {},
   "source": [
    "**Challenges:**\n",
    "\n",
    "- The code failed if the API key was incorrect or if there was a network issue.\n",
    "- There was no feedback about why the request failed, making it hard to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a47bf-2baa-48a3-aa29-09f3e7b5ca70",
   "metadata": {},
   "source": [
    "### Improved Solution:\n",
    "By adding `try-except` blocks, I was able to handle potential HTTP errors and connection failures gracefully. This ensured the program continued running and provided meaningful feedback about the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a2f52-5612-41e2-8df1-813b1f2112d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved code with error handling\n",
    "try:\n",
    "    response = requests.get(api_url, params=params)\n",
    "    response.raise_for_status()  # Raises an error for bad responses\n",
    "    weather_data = response.json()\n",
    "    print(weather_data)\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except requests.exceptions.RequestException as req_err:\n",
    "    print(f\"Request error occurred: {req_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21517-76e1-4511-a754-01fee51e675e",
   "metadata": {},
   "source": [
    "**Reflection:** Adding error handling significantly improved the reliability of the API requests. Now, even if the request fails, the program continues running and provides meaningful error messages, making debugging easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d369ca5-51df-415d-9cb7-15342627c5c9",
   "metadata": {},
   "source": [
    "## 3. SQL and Python Integration with SQLAlchemy ORM\n",
    "**Explanation**: Initially, I wrote long SQL queries as raw strings in Python, which made the code difficult to maintain and debug, especially with complex queries. By switching to SQLAlchemy ORM, I could write Pythonic queries, making the code more readable and maintainable.\n",
    "\n",
    "### Initial Attempt:\n",
    "This approach used raw SQL strings, which became cumbersome with larger queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7443bb5-eada-481c-bbff-d3b0401ba095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Assuming the engine is connected to the database\n",
    "engine = create_engine('postgresql://user:password@localhost/mydatabase')\n",
    "\n",
    "# Raw SQL query\n",
    "query = \"\"\"\n",
    "SELECT customers.name, orders.amount \n",
    "FROM customers \n",
    "JOIN orders ON customers.id = orders.customer_id\n",
    "\"\"\"\n",
    "result = engine.execute(query)\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a79ef1-68c4-4bcf-8e53-5c3d5e51dbd0",
   "metadata": {},
   "source": [
    "**Challenges:**\n",
    "\n",
    "- Raw SQL mixed with Python code made the script less readable.\n",
    "- It became challenging to debug complex queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4b56a-7f8c-4210-9711-81ca964fea76",
   "metadata": {},
   "source": [
    "### Improved Solution:\n",
    "Switching to SQLAlchemy ORM (Object Relational Mapper) allowed me to work with Pythonic queries instead of raw SQL. This improved both readability and maintainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969daef-afb0-484b-8497-e3c853265d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from models import Customer, Order  # Assuming models are defined\n",
    "\n",
    "# Set up session with database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Using SQLAlchemy ORM to query data\n",
    "result = session.query(Customer.name, Order.amount).join(Order).all()\n",
    "\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9454f-d6dd-4bd7-9742-78f405079f48",
   "metadata": {},
   "source": [
    "**Reflection:** Using SQLAlchemy ORM made the code more concise and Pythonic, while reducing the risk of syntax errors. It also provided better structure for more complex queries, making the overall workflow cleaner and easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb47e4-9ab6-402a-9653-32c81c7cad6b",
   "metadata": {},
   "source": [
    "## 4. Integrating SQL with Python in Jupyter Notebook\n",
    "**Explanation**: Initially, I used separate environments for SQL querying and Python analysis, which made the workflow disjointed. By integrating SQL directly into the Jupyter Notebook using `ipython-sql`, I could run SQL queries and analyse the data in the same environment, streamlining the workflow.\n",
    "\n",
    "### Initial Attempt:\n",
    "The SQL query and Python analysis were done in separate environments, requiring manual data export/import.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04b259-b0de-46ce-86f6-01fc69752264",
   "metadata": {},
   "source": [
    "### Improved Solution:\n",
    "Using `%load_ext sql` and `%%sql` within Jupyter Notebooks allowed me to run SQL queries directly within the notebook, improving efficiency and maintaining the workflow in a single environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76950337-7627-4475-ac6c-754ddf8e1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ipython-sql extension\n",
    "%load_ext sql\n",
    "%sql postgresql://user:password@localhost/mydatabase\n",
    "\n",
    "# Write SQL query directly in Jupyter notebook\n",
    "%%sql\n",
    "SELECT * FROM sales WHERE product = 'Laptop';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3e044-355e-44b3-8fda-f6a12b719810",
   "metadata": {},
   "source": [
    "**Reflection:** Ensuring that the correct dependencies are installed in the environment was key to resolving this issue. Once I resolved this, database connectivity worked seamlessly, and I was able to execute queries as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7af3c-f26a-4f9d-8df9-9cb4241f8e7e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, I explored several key areas related to data access and manipulation, including the use of SQLAlchemy with PostgreSQL and the integration of APIs with Python.\n",
    "\n",
    "### 1. **Web Scraping with BeautifulSoup**\n",
    "I began by scraping a webpage using BeautifulSoup and Requests to extract relevant data such as links. Initially, I faced challenges filtering irrelevant data, which I resolved by targeting specific HTML tags and class attributes. Using tools like BeautifulSoup allows me to automate data collection from static websites efficiently.\n",
    "\n",
    "### 2. **Working with APIs (OpenWeather Example)**\n",
    "Next, I demonstrated how to connect to an external API, using the OpenWeather API as an example. I addressed key concepts such as:\n",
    "- **Authentication** using API keys (`appid` for OpenWeather).\n",
    "- How to handle **HTTP request errors** such as `401 Unauthorised` by ensuring the correct API key is in use.\n",
    "This section also highlighted the importance of handling response codes to make the process more robust.\n",
    "\n",
    "### 3. **Database Connectivity using SQLAlchemy and PostgreSQL**\n",
    "The most important aspect of this notebook was establishing a database connection using **SQLAlchemy** and **psycopg2** as the PostgreSQL driver. I discussed how to:\n",
    "- **Set up the database connection string** by gathering information such as the database name, host, user, and password.\n",
    "- **Install psycopg2** to enable PostgreSQL communication and address the common `ModuleNotFoundError` issue.\n",
    "- Run **raw SQL queries** to interact with the database and retrieve useful data.\n",
    "\n",
    "By integrating SQLAlchemy with PostgreSQL, I gained advantage of writing both Pythonic code and raw SQL, allowing for flexible data management across applications. \n",
    "\n",
    "### 4. **Challenges and Solutions**\n",
    "Throughout the notebook, several challenges were encountered:\n",
    "- **ModuleNotFoundError** for psycopg2, which was resolved by installing the correct package in the appropriate environment.\n",
    "- Issues with API authentication, resolved by ensuring correct API key usage and handling response codes for better feedback.\n",
    "- Managing database connections by ensuring the correct user credentials and host settings were used for the PostgreSQL connection.\n",
    "\n",
    "### 5. **Conclusion**\n",
    "This notebook provided a step-by-step guide on how to use Python effectively for web scraping, interacting with external APIs, and querying databases. By combining the strengths of libraries like Requests, BeautifulSoup, and SQLAlchemy, I have created a flexible workflow that allows us to collect, analyse, and manipulate data from various sources efficiently. Moving forward, it’s essential to build on these fundamentals, incorporating advanced error handling, automation, and scalability into the data workflow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
